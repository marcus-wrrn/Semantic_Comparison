{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "from os.path import isfile\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import Tools.processing as proc\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Five_Stars</th>\n",
       "      <th>One_Star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0196684411, 0.0089247576, 0.0033987069, -0....</td>\n",
       "      <td>[0.0130684374, 0.0054455661, -0.0202282401, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Five_Stars  \\\n",
       "0  [0.0196684411, 0.0089247576, 0.0033987069, -0....   \n",
       "\n",
       "                                            One_Star  \n",
       "0  [0.0130684374, 0.0054455661, -0.0202282401, -0...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon_pooled = pd.read_json(\"./Data_Storage/Processed_Data/Amazon_Pooled.json\")\n",
    "\n",
    "df_amazon_pooled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get positive and negative semantic embeddings\n",
    "positive_amazon_enc = df_amazon_pooled[\"Five_Stars\"][0]\n",
    "negative_amazon_enc = df_amazon_pooled[\"One_Star\"][0]\n",
    "\n",
    "# Load test and train sets\n",
    "titles_df = [\"Text\", \"Score\"]\n",
    "df_amazon = pd.read_csv(\"/home/marcuswrrn/Projects/Semantic_Quantification/Semantic_Comparison/Lab_Tests/Data_Storage/Positive_Scores/sentiment labelled sentences/amazon_cells_labelled.txt\", names=titles_df, sep='\\t')\n",
    "df_imbd = pd.read_csv(\"/home/marcuswrrn/Projects/Semantic_Quantification/Semantic_Comparison/Lab_Tests/Data_Storage/Positive_Scores/sentiment labelled sentences/imdb_labelled.txt\", names=titles_df, sep='\\t')\n",
    "df_yelp = pd.read_csv(\"/home/marcuswrrn/Projects/Semantic_Quantification/Semantic_Comparison/Lab_Tests/Data_Storage/Positive_Scores/sentiment labelled sentences/yelp_labelled.txt\", names=titles_df, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_amazon = df_amazon[\"Text\"].apply(lambda x: model.encode(x))\n",
    "encodings_imbd = df_imbd[\"Text\"].apply(lambda x: model.encode(x))\n",
    "encodings_yelp = df_yelp[\"Text\"].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(embed, positive=positive_amazon_enc, negative=negative_amazon_enc, hyper=0.2, visible=False):\n",
    "    embed_temp = np.array([float(x) for x in embed], dtype=np.float32)\n",
    "    positive = np.array([float(x) for x in positive], dtype=np.float32)\n",
    "    negative = np.array([float(x) for x in negative], dtype=np.float32)\n",
    "\n",
    "    similarity_pos = util.cos_sim(embed_temp, positive)[0][0]\n",
    "    similarity_neg = util.cos_sim(embed_temp, negative)[0][0]\n",
    "    #print(f\"{similarity_pos} : {similarity_neg} : {similarity_pos >= similarity_neg}\")\n",
    "    diff = similarity_pos - (similarity_neg + (similarity_pos *hyper))\n",
    "    if visible:\n",
    "        print(f\"Difference in embedding {diff}\")\n",
    "    return (diff >= 0).item()\n",
    "\n",
    "\n",
    "df_amazon[\"Train_Score\"] = encodings_amazon.apply(lambda x: compare_embeddings(x, hyper=0))\n",
    "df_imbd[\"Train_Score\"] = encodings_imbd.apply(lambda x: compare_embeddings(x, hyper=0))\n",
    "df_yelp[\"Train_Score\"] = encodings_yelp.apply(lambda x: compare_embeddings(x, hyper=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon\n",
      "[[453.  47.]\n",
      " [366. 134.]]\n",
      "Precision: 0.7403314917127072\n",
      "Accuracy: 0.587\n",
      "Recall: 0.268\n",
      "\n",
      "IMBD\n",
      "[[285.  77.]\n",
      " [177. 209.]]\n",
      "Precision: 0.7307692307692307\n",
      "Accuracy: 0.660427807486631\n",
      "Recall: 0.5414507772020726\n",
      "\n",
      "Yelp\n",
      "[[286. 214.]\n",
      " [142. 358.]]\n",
      "Precision: 0.6258741258741258\n",
      "Accuracy: 0.644\n",
      "Recall: 0.716\n"
     ]
    }
   ],
   "source": [
    "def build_confusion_matrix(train_scores, test_scores):\n",
    "    matrix = np.zeros((2, 2))\n",
    "    for train, test in zip(train_scores, test_scores):\n",
    "        train_temp = 1 if train else 0\n",
    "        test_temp = 1 if test else 0\n",
    "        #print(f\"Train: {train}, Test: {test}\")\n",
    "        matrix[test_temp][train_temp] += 1\n",
    "    return matrix\n",
    "\n",
    "def find_test_score(train_score, test_score):\n",
    "    count = 0\n",
    "    for test, train in zip(test_score, train_score):\n",
    "        count += 1 if test == train else 0\n",
    "    return count/len(test_score)\n",
    "\n",
    "def binary_confusion_matrix_scores(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Compute precision, accuracy, and recall scores from a 2x2 confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        confusion_matrix (list of lists): A 2x2 list containing the confusion matrix, \n",
    "                                          with rows representing true labels and columns representing predicted labels.\n",
    "    \n",
    "    Returns:\n",
    "        precision (float): Precision score for the positive class.\n",
    "        accuracy (float): The overall accuracy of the classifier.\n",
    "        recall (float): Recall score for the positive class.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure the input is a 2x2 matrix\n",
    "    if len(confusion_matrix) != 2 or any(len(row) != 2 for row in confusion_matrix):\n",
    "        raise ValueError(\"The confusion matrix must be 2x2\")\n",
    "    \n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives = confusion_matrix[1][1]\n",
    "    false_positives = confusion_matrix[0][1]\n",
    "    false_negatives = confusion_matrix[1][0]\n",
    "    true_negatives = confusion_matrix[0][0]\n",
    "    \n",
    "    # Compute precision, accuracy, and recall scores\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0\n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "def print_test_log(df : pd.DataFrame, train_label=\"Train_Score\", test_label=\"Score\"):\n",
    "    \n",
    "    conf_matrix = build_confusion_matrix(df[train_label], df[test_label])\n",
    "    print(conf_matrix)\n",
    "    binary_confusion_matrix_scores(conf_matrix)\n",
    "\n",
    "def print_all_logs(df_amazon, df_imbd, df_yelp):\n",
    "    print(\"Amazon\")\n",
    "    print_test_log(df_amazon)\n",
    "    print()\n",
    "    print(\"IMBD\")\n",
    "    print_test_log(df_imbd)\n",
    "    print()\n",
    "    print(\"Yelp\")\n",
    "    print_test_log(df_yelp)\n",
    "\n",
    "\n",
    "\n",
    "print_all_logs(df_amazon, df_imbd, df_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>Train_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score  Train_Score\n",
       "0  A very, very, very slow-moving, aimless movie ...      0        False\n",
       "1  Not sure who was more lost - the flat characte...      0        False\n",
       "2  Attempting artiness with black & white and cle...      0        False\n",
       "3       Very little music or anything to speak of.        0        False\n",
       "4  The best scene in the movie was when Gerardo i...      1        False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imbd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_mean(list_of_lists):\n",
    "    # Convert the input list of lists to a numpy array\n",
    "    data = np.array(list_of_lists)\n",
    "    # Compute the mean average along the columns (axis=0)\n",
    "    mean_average = np.mean(data, axis=0)\n",
    "    # Convert the result back to a Python list\n",
    "    mean_average_list = mean_average.tolist()\n",
    "    return mean_average_list\n",
    "\n",
    "def get_avg_embeds(df: pd.DataFrame, encodings: list):\n",
    "    df['Encodings'] = encodings\n",
    "    # Lets average all embeddings\n",
    "    df_pos = df[df['Score'] == 1].reset_index()\n",
    "    df_neg = df[df['Score'] == 0].reset_index()\n",
    "    return column_mean(df_pos['Encodings']), column_mean(df_neg['Encodings'])\n",
    "\n",
    "\n",
    "positive_imbd_enc, negative_imbd_enc = get_avg_embeds(df_imbd, encodings_imbd)\n",
    "positive_amazon_enc, negative_amazon_enc = get_avg_embeds(df_amazon, encodings_amazon)\n",
    "positive_yelp_enc, negative_yelp_enc = get_avg_embeds(df_yelp, encodings_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= Dataset: Amazon =================\n",
      "Amazon\n",
      "[[443.  57.]\n",
      " [ 46. 454.]]\n",
      "Precision: 0.8884540117416829\n",
      "Accuracy: 0.897\n",
      "Recall: 0.908\n",
      "\n",
      "IMBD\n",
      "[[358.   4.]\n",
      " [ 50. 336.]]\n",
      "Precision: 0.9882352941176471\n",
      "Accuracy: 0.9278074866310161\n",
      "Recall: 0.8704663212435233\n",
      "\n",
      "Yelp\n",
      "[[473.  27.]\n",
      " [ 27. 473.]]\n",
      "Precision: 0.946\n",
      "Accuracy: 0.946\n",
      "Recall: 0.946\n",
      "\n",
      "================= Dataset: IMDb =================\n",
      "Amazon\n",
      "[[465.  35.]\n",
      " [ 63. 437.]]\n",
      "Precision: 0.9258474576271186\n",
      "Accuracy: 0.902\n",
      "Recall: 0.874\n",
      "\n",
      "IMBD\n",
      "[[343.  19.]\n",
      " [ 22. 364.]]\n",
      "Precision: 0.9503916449086162\n",
      "Accuracy: 0.9451871657754011\n",
      "Recall: 0.9430051813471503\n",
      "\n",
      "Yelp\n",
      "[[478.  22.]\n",
      " [ 23. 477.]]\n",
      "Precision: 0.9559118236472945\n",
      "Accuracy: 0.955\n",
      "Recall: 0.954\n",
      "\n",
      "================= Dataset: Yelp =================\n",
      "Amazon\n",
      "[[462.  38.]\n",
      " [ 65. 435.]]\n",
      "Precision: 0.919661733615222\n",
      "Accuracy: 0.897\n",
      "Recall: 0.87\n",
      "\n",
      "IMBD\n",
      "[[358.   4.]\n",
      " [ 58. 328.]]\n",
      "Precision: 0.9879518072289156\n",
      "Accuracy: 0.9171122994652406\n",
      "Recall: 0.8497409326424871\n",
      "\n",
      "Yelp\n",
      "[[468.  32.]\n",
      " [ 23. 477.]]\n",
      "Precision: 0.93713163064833\n",
      "Accuracy: 0.945\n",
      "Recall: 0.954\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        name = \"Amazon\"\n",
    "        positive, negative = positive_amazon_enc, negative_amazon_enc\n",
    "    elif i == 1:\n",
    "        name = \"IMDb\"\n",
    "        positive, negative = positive_imbd_enc, negative_imbd_enc\n",
    "    else:\n",
    "        name = \"Yelp\"\n",
    "        positive, negative = positive_yelp_enc, negative_yelp_enc\n",
    "\n",
    "    df_amazon[\"Train_Score\"] = encodings_amazon.apply(lambda x: compare_embeddings(x, positive=positive, negative=negative, hyper=0))\n",
    "    df_imbd[\"Train_Score\"] = encodings_imbd.apply(lambda x: compare_embeddings(x, positive=positive, negative=negative, hyper=0))\n",
    "    df_yelp[\"Train_Score\"] = encodings_yelp.apply(lambda x: compare_embeddings(x, positive=positive, negative=negative, hyper=0))\n",
    "    print(f\"\\n================= Dataset: {name} =================\")\n",
    "    print_all_logs(df_amazon, df_imbd, df_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon\n",
      "[[462.  38.]\n",
      " [ 65. 435.]]\n",
      "Precision: 0.919661733615222\n",
      "Accuracy: 0.897\n",
      "Recall: 0.87\n",
      "\n",
      "IMBD\n",
      "[[358.   4.]\n",
      " [ 58. 328.]]\n",
      "Precision: 0.9879518072289156\n",
      "Accuracy: 0.9171122994652406\n",
      "Recall: 0.8497409326424871\n",
      "\n",
      "Yelp\n",
      "[[468.  32.]\n",
      " [ 23. 477.]]\n",
      "Precision: 0.93713163064833\n",
      "Accuracy: 0.945\n",
      "Recall: 0.954\n"
     ]
    }
   ],
   "source": [
    "print_all_logs(df_amazon, df_imbd, df_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt_pooled = pd.read_json(\"/home/marcuswrrn/Projects/Semantic_Quantification/Semantic_Comparison/Data_Processing/Data/job_pooled_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon\n",
      "[[451.  49.]\n",
      " [ 49. 451.]]\n",
      "Precision: 0.902\n",
      "Accuracy: 0.902\n",
      "Recall: 0.902\n",
      "\n",
      "IMBD\n",
      "[[351.  11.]\n",
      " [ 25. 361.]]\n",
      "Precision: 0.9704301075268817\n",
      "Accuracy: 0.9518716577540107\n",
      "Recall: 0.9352331606217616\n",
      "\n",
      "Yelp\n",
      "[[471.  29.]\n",
      " [ 21. 479.]]\n",
      "Precision: 0.9429133858267716\n",
      "Accuracy: 0.95\n",
      "Recall: 0.958\n"
     ]
    }
   ],
   "source": [
    "positive = column_mean([positive_imbd_enc, positive_amazon_enc])\n",
    "negative = column_mean([negative_imbd_enc, negative_amazon_enc])\n",
    "\n",
    "df_amazon[\"Train_Score\"] = encodings_amazon.apply(lambda x: compare_embeddings(x, positive=positive, negative=negative, hyper=0))\n",
    "df_imbd[\"Train_Score\"] = encodings_imbd.apply(lambda x: compare_embeddings(x, positive=positive, negative=negative, hyper=0))\n",
    "df_yelp[\"Train_Score\"] = encodings_yelp.apply(lambda x: compare_embeddings(x, positive=positive, negative=negative, hyper=0))\n",
    "\n",
    "\n",
    "\n",
    "print_all_logs(df_amazon, df_imbd, df_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive/Negative Tests:\n",
      "tensor(0.1606)\n",
      "tensor(0.1677)\n",
      "\n",
      "Neutral Testing:\n",
      "tensor(0.0594)\n",
      "tensor(0.0435)\n"
     ]
    }
   ],
   "source": [
    "pos = [float(x) for x in df_gpt_pooled[\"positive\"]]\n",
    "neg = [float(x) for x in df_gpt_pooled[\"negative\"]]\n",
    "neut = [float(x) for x in df_gpt_pooled[\"neutral\"]]\n",
    "\n",
    "print(\"Positive/Negative Tests:\")\n",
    "print(util.cos_sim(positive, pos)[0][0])\n",
    "print(util.cos_sim(negative, neg)[0][0])\n",
    "\n",
    "print(\"\\nNeutral Testing:\")\n",
    "print(util.cos_sim(positive, neut)[0][0])\n",
    "print(util.cos_sim(negative, neut)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1814)\n",
      "tensor(0.1421)\n",
      "tensor(0.1236)\n"
     ]
    }
   ],
   "source": [
    "sent = \"I'm trapped in a wonderful dream\"\n",
    "enc = model.encode(sent)\n",
    "\n",
    "print(util.cos_sim(positive, enc)[0][0])\n",
    "print(util.cos_sim(negative, enc)[0][0])\n",
    "print(util.cos_sim(neut, enc)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_pos = [df_gpt_pooled[\"positive\"], positive_yelp_enc]\n",
    "encodings_neg = [df_gpt_pooled[\"negative\"], negative_yelp_enc]\n",
    "\n",
    "encodings_pos = column_mean(encodings_pos)\n",
    "encodings_neg = column_mean(encodings_neg)\n",
    "\n",
    "df_amazon[\"Train_Score\"] = encodings_amazon.apply(lambda x: compare_embeddings(x, positive=encodings_pos, negative=encodings_neg, hyper=0))\n",
    "df_imbd[\"Train_Score\"] = encodings_imbd.apply(lambda x: compare_embeddings(x, positive=encodings_pos, negative=encodings_neg, hyper=0))\n",
    "df_yelp[\"Train_Score\"] = encodings_yelp.apply(lambda x: compare_embeddings(x, positive=encodings_pos, negative=encodings_neg, hyper=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
